\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{subcaption}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{statement}{Statement}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\usepackage{graphicx}
\graphicspath{ {../images/} }

\begin{document}
		\section{Introduction}
		The aim of the paper is to design the framework to learn the target distribution based on random process.
		
		\section{Target data}
		\subsection{Geometric Brownian Motion}
		
			%\subsection{Brownian path}
				The stochastic process $X_t$ is said to follow GBM if it satisfying the following SDE
				
				\begin{equation}
					dX_t = \mu X_tdt + \sigma X_t dW_t 
				\end{equation}
				
				$W$ is the Brownian motion which determine the process from beginning $S_{t=0}$ to $S_{t=T}$.
				
				\begin{equation}
					W_{k} = \sum_{t=1}^{k} b_{t}, \ \ k = 1, \dots, m
				\end{equation}
				 where $b$ is the added randomness to the model.
				 which stores a random number coming from the standard normal distribution $N(0, 1)$.
			
			The solution of above SDE has the analytic solution
			
			\begin{equation}
				S_{k} = S_{0} \prod_{i=1}^{k}e^{\left(\mu-\frac{1}{2}\sigma^{2}\right)t+\sigma W(t)}
				\label{eq:bm_process}
			\end{equation}
			
			
				
				
	
	\subsection{Ornstein-Uhlenbeck process}
	The Ornstein-Uhlenbeck process differential equation is given by
	\begin{equation}
		dX_t = aX_tdt + \sigma dW_t 
		\label{eq:ornuh_diff}
	\end{equation}
	An additional drift term is sometimes added:
	\begin{equation}
	dX_t = \sigma dW_t +a (\mu-x_t)dt
	\label{eq:ornuh_diff_mean}
	\end{equation}
	where $\sigma$ and $a$ is constants and $\{W_t, t \leq 0\}$ is a standard Brownian motion.
	\begin{equation}
		X_t = e^{at}X_0 + \sigma \int_{0}^{t}e^{a(s-t)}dW_s
			\end{equation}
	
	To approximate the numerical solution, we use Euler-Maruyama method to estimate $X_t$.
	
	Take (\ref{eq:ornuh_diff_mean}) for example. 
	First, partition the interval $[0,T]$ into $N$ equidistance sunintervals. 
	Then, recursively solving $X_{n+1}$ by
	\begin{equation}
		X_{n+1} = X_{n} + a (\mu-x_t) \bigtriangleup t+ \sigma \bigtriangleup dW_s
	\end{equation}
	where $\bigtriangleup W_n$ is obtained by sampling a random number from normal distribution with expected value zero and variance $\bigtriangleup t$.
	
	
	
	%If the noise data comes from independent data i.e. normal distribution, unique distribution, the discriminator easily discriminate the noise data as fake example.
	%Thus, the real examples $x$ easily beat the fake examples.
	%which degrades the quality of the output. 
	
	%However, if the noise data is brownian motion, the distribution of GAN is stable. 
	%Because each element of vector in brownian motion is also dependent, it easily fool GAN to believe the noise data comes from real data instead generated by $G$. 
	
	
	\section{Model}
	\subsection{Generative Adversarial Networks (GANs)}
	
	During the training, discriminator $D$ and generator $G$ play the following two-player minimax game during the training. 
	 The discriminator takes the input data from real and fake data and calculate the score individually to discriminate whether a data sample comes from real data or generated by generator. 
	 The score calcualted by discriminator represents the probability of the input comes from real data.  
	 The generator takes the random noise data $z$ as inout.
	 It tries to output the data samples from distribution to fool the discriminator.
	 
	 
	The optimizing approach for GAN is based on \cite{goodfellow} which is composed with two loss function.
	One aim to increase the value of $D(s)$ to let the discriminator believe the data $s$ comes from real data.
	The other one aims to train generator $G$ to maximize the $D(G(z))$. 

	In the first inner loop of training, 
	it updates the discriminator by ascending its stochastic gradient:
	\begin{equation}
		\bigtriangledown_{x_d} \frac{1}{m} \sum_{t=1}^{m} \left[ \log D(x^{\left(t\right)}) + \log \left(1-D\left(G\left(z^{(t)}\right)\right)\right)\right]
	\end{equation}
	
	After finishing the first inner loop of training, it updates the generator by descending its stochastic gradient with only one step:
	\begin{equation}
		\bigtriangledown_{x_g} \frac{1}{m} \sum_{t=1}^{m} \log \left(1-D\left(G\left(z^{(t)}\right)\right)\right)
		\label{fig:loss_gan}
	\end{equation}
	where $x_d$ and $x_g$ represents the variables for discriminator and generator, $t$ indicates the time for each vector and $z^{(t)}$ is noise sample.
	As the value of $D(G(z))$ increase, discriminator evaluate the output of $G$ is real data. 
	
	\subsubsection{Noise data}
	Generator  takes the random noise data as input data. 
	Then, it captures the distribution of real examples to imitate the data sample from real data. 
	We found out that the initial distribution of noise data for generator influence the performance of GAN. 
	
	The results of the GAN's output distribution are shown in Figure \ref{fig:dstr_bm} and \ref{fig:dstr_nm}. The path simulation is show in Figure \ref{fig:path_gan}. The detail of procedure for experiment is in section 3. 
	
	\subsection{Wassersein GAN}
	The framework of Wassersein-GAN(WGAN) is provided by \cite{arjovsky}. 
	It is an alternative to traditional GAN training.
	The mode is trained to minimize the Earth-Mover(EM) distance between real distribution and model distribution.
	The EM distance between distribution of two point processes is:
	\begin{equation}
		W(\mathbb{P}_r, \mathbb{P}_g) = \inf_{\gamma \in \prod(\mathbb{P}_r, \mathbb{P}_g)} \mathbb{E}_{(x,y)\sim \gamma}\left[||x-y||\right]
	\end{equation}
	
	Since the distance is continuous and differentiable, it can can be trained until optimally. 
	
	
	\section{Empirical results}
	\subsection{The simulation of Geometric Brownian Motion by GAN}
	
	Given the real data $\{ s(i)\in A \}, i = 1, \dots, 10$ for all $A \subset \mathbb{R}^{10}$.
	The real data is geometric Brownian motion which is generated by following (\ref{eq:bm_process}).
	 We implemented two experiments to investigate the effect of noise data for the model distribution.
	The first experiment takes the independent process as noise data the other one take dependent process.
	Then, we estimated the probability of the each dimensional data under generator’s distribution by fitting lognormal distribution to the samples generated with $G$. 
		
	In the first experiment, the generator nets used normal distribution as input to represented independent process.
	Results are reported in Figure \ref{fig:dstr_nm}. 
	In Figure \ref{fig:dstr_nm}, we compared the real distribution with model distribution for each dimension after training. 
	We observe that the model distribution does not match with real distribution in some dimension. 
	
	To compare two distribution statistically, we implemented two-sample Anderson–Darling test the hypothesis that two samples are drawn from identical distribution in both experiment.
	First, we combined data set $Z$ and ordered it ascending.
	Then, the two-sample Anderson-Darling test statistic is calculated by
	
	\begin{equation}
	AD = \frac{1}{mn}\sum_{i=1}^{n+m}\frac{(N_iZ_{(n+m)}-ni)^2}{iZ_{n+m-i}}
	\end{equation}
	
	where $n$ and $m$ is the number of observations in two samples respectively and $N_i$ represents the number of observations in $z_n$ that are equal to or smaller
	than the ith observation in $Z_{(n+m)}$. 
	
	The AD value is over 100 which is larger than the correspondent critical value.  
	Hence, the null hypothesis that two samples come from same distribution is rejected. 
	Therefore, we conclude that the the model distribution does not have the same distribution of geometric brownian motion.
	
	In the second experiment, 
	since the Geometric Brownian Motion is dependent, we assume the discriminator is easily fooled by another dependent process.
	Thus, we choose Brownian Motion to be the input of generator to see whether it can improve the simulation ability.

	Results are reported in Figure \ref{fig:dstr_bm}. 
	The improvement is not significantly.
	The AD values is also over 100, so we reject the hypothesis that two samples come from same distribution in the experiment. 
	
	\begin{figure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr0_nm_gan.png}
			%\caption{Noise data generated from normal distribution}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr1_nm_gan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr2_nm_gan.png}
			%\caption{Noise data generated from normal distribution}
		\end{subfigure}
		
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr3_nm_gan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr4_nm_gan.png}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr5_nm_gan.png}
		\end{subfigure}
		
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr6_nm_gan.png}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr7_nm_gan.png}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr8_nm_gan.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr9_nm_gan.png}
		\end{subfigure}
		\caption{Distribution in specific day with Brownian motion as the input for generator. The histogram is the distribution of sampling generated from generator. The black line is the distribution for the real data in each dimension.}
		\label{fig:dstr_nm}
	\end{figure}

	\begin{figure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr0_bm_gan.png}
			%\caption{Noise data generated from normal distribution}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr1_bm_gan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr2_bm_gan.png}
			%\caption{Noise data generated from normal distribution}
		\end{subfigure}
		
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr3_bm_gan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr4_bm_gan.png}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr5_bm_gan.png}
		\end{subfigure}
		
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr6_bm_gan.png}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr7_bm_gan.png}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr8_bm_gan.png}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{dstr9_bm_gan.png}
		\end{subfigure}
		\caption{Distribution in specific day with Brownian motion as the input for generator. The histogram is the distribution of sampling generated from generator. The black line is the distribution for real data in each dimension.}
		\label{fig:dstr_bm}
	\end{figure}

	\subsubsection{Simulation Results}

	Figure \ref{fig:path_gan} plots path simulation.
	It is easily observe from figure that it occurs model collapse in GAN in two experiments.
	It is more obvious if the noise data is based on independent process. 
	Some of paths dominate the generative sampling while training.
	The generator was be pushed towards producing samples that come from that specific mode, ignoring the other classes most of time. 

	\begin{figure}
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[width=\textwidth]{path_gbm.png}
			\caption{Geometric Brownian Motion}
		\end{subfigure}
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[width=\textwidth]{path_gan_nm.png}
			\caption{Independent noise data: normal distribution}
		\end{subfigure}
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[width=\textwidth]{path_gan_bm.png}
			\caption{Dependent noise data: Brownian Motion}
		\end{subfigure}
	\caption{Path Simulation. It is easily to observe that there is model collapse while training GAN.}
	\label{fig:path_gan}
	\end{figure}

	\subsection{The simulation of Geometric Brownian Motion by WGAN}


	
	

	
	
	
	
	\bibliographystyle{unsrt}
	\bibliography{gan-BM}
\end{document}